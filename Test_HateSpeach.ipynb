{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kCZQLOmg2WE"
      },
      "source": [
        "This is the drive link\n",
        "https://drive.google.com/drive/folders/14SVAWYF0mz62BXiEaqMHDvXliEaq-Wto?usp=sharing\n",
        "\n",
        "LSTM TEST-RUN THE FIRST CELL\n",
        "BERT TEST-RUN THE SECONED CELL             \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgy3efVXfHPx",
        "outputId": "a596241b-5e7c-4628-8af1-7d982d6593db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "Testing...: 100%|██████████| 625/625 [00:09<00:00, 62.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2817416494011879, Test Accuracy: 0.8791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Run this cell to Test LstmNetwork\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class HateSpeechDataset(Dataset):\n",
        "    def __init__(self, dataframe, vocab_to_int, max_length):\n",
        "        self.data = dataframe\n",
        "        self.vocab_to_int = vocab_to_int\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        Content = self.data.iloc[idx]['Content']\n",
        "        label = self.data.iloc[idx]['Label']\n",
        "        encode = [self.vocab_to_int[seq] for seq in Content.split() if seq in self.vocab_to_int]\n",
        "        if len(encode) < self.max_length:\n",
        "            encode += [0] * (self.max_length - len(encode))  #padding\n",
        "        else:\n",
        "            encode = encode[:self.max_length]\n",
        "\n",
        "        seq_length = min(len(encode), self.max_length)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encode, dtype=torch.long),\n",
        "            'length': torch.tensor(seq_length, dtype=torch.long),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class Attention_layer(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention_layer, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        output = self.attention(lstm_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        output = torch.sum(output * lstm_output, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n",
        "        super(LSTMNet, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.dropout_l = nn.Dropout(dropout)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True,dropout=0.1)\n",
        "\n",
        "        self.attention = Attention_layer(hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(2*hidden_dim , output_dim)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "        output = self.embedding(input_ids)\n",
        "        output = self.dropout_l(output)\n",
        "\n",
        "        output = pack_padded_sequence(output, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        output,_  = self.lstm(output)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        output = self.attention(output)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_samples = 0\n",
        "    n_sampels = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training...\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        lengths = batch['length'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_samples += torch.sum(predicted == labels).item()\n",
        "        n_sampels += labels.size(0)\n",
        "\n",
        "    acc = correct_samples / n_sampels\n",
        "    avg_train_loss = total_loss / len(dataloader)\n",
        "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {acc}\")\n",
        "    return avg_train_loss, acc\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "\n",
        "def test(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_samples = 0\n",
        "    n_sampels = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Testing...\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            lengths = batch['length'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_samples += torch.sum(predicted == labels).item()\n",
        "            n_sampels += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = correct_samples / n_sampels\n",
        "    print(f\"Test Loss: {avg_loss}, Test Accuracy: {acc}\")\n",
        "    return avg_loss, acc\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = pd.read_csv(\"HateSpeechDatasetBalanced.csv\")\n",
        "\n",
        "train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=30)\n",
        "test_dataset = test_dataset.sample(n=10000, replace=False, random_state=30)\n",
        "\n",
        "\n",
        "embedding_dim = 300\n",
        "hidden_dim = 128\n",
        "output_dim = 2\n",
        "num_epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "vocab_counter = Counter()\n",
        "max_length = 500\n",
        "for text in train_dataset['Content']:\n",
        "    words = text.split()\n",
        "    vocab_counter.update(words)\n",
        "\n",
        "vocab_to_int = {word: i for i, (word, _) in enumerate(vocab_counter.items(), 1)}\n",
        "vocab_size = len(vocab_to_int) + 1\n",
        "\n",
        "train_dataset = HateSpeechDataset(train_dataset, vocab_to_int, max_length)\n",
        "test_dataset = HateSpeechDataset(test_dataset, vocab_to_int, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "lstm_model = LSTMNet(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "lstm_model.load_state_dict(torch.load(\"LSTMTrained.pth\", map_location=device))\n",
        "_=test(lstm_model, test_loader, criterion, device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oriyO-dBgqwD",
        "outputId": "a98b54d2-ed23-49cf-8831-f397183fa8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Testing...: 100%|██████████| 625/625 [01:21<00:00,  7.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.24458147599548102, Test Accuracy: 0.8998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "######################\n",
        "###Run this to test the bert ###\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define  hate speech detection Dataset class ,this Class have the same\n",
        "# idea  of the class that we used in hw2 (help us in go) give us in hw2\n",
        "#this class helps in preparing text data for hate speech detection tasks\n",
        "class Hate_speech(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        x=len(self.data)\n",
        "        return x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx]['Content']\n",
        "        label = self.data.iloc[idx]['Label']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "#defin the BERT_Net\n",
        "class FTB_Net(nn.Module):\n",
        "    def __init__(self, Bert_model):\n",
        "        super(FTB_Net, self).__init__()\n",
        "        self.Bert = Bert_model\n",
        "        self.fc = nn.Linear(Bert_model.config.hidden_size, 2)\n",
        "\n",
        "        for param in self.Bert.parameters():     #turn off all hidden layers - Freeze\n",
        "            param.requires_grad = False\n",
        "        for param in self.Bert.encoder.layer[-2:].parameters(): #turn on last 2 layers - Fine tune\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        output = self.Bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        output = output.pooler_output\n",
        "        output = self.fc(output)        #classifaction layer\n",
        "        return output\n",
        "\n",
        "\n",
        "# train and test functions\n",
        "def train(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_samples = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training...\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        correct_samples += torch.sum(predictions == labels).item()\n",
        "        n_samples += labels.size(0)\n",
        "\n",
        "    acc = correct_samples / n_samples\n",
        "    avg_train_loss = total_loss / len(dataloader)\n",
        "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {acc}\")\n",
        "\n",
        "    # Append metrics for plotting\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(acc)\n",
        "\n",
        "def test(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_samples = 0\n",
        "    n_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Testing...\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct_samples += torch.sum(predictions == labels).item()\n",
        "            n_samples += labels.size(0)\n",
        "\n",
        "    avg_test_loss = total_loss / len(dataloader)\n",
        "    acc = correct_samples / n_samples\n",
        "    print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {acc}\")\n",
        "\n",
        "    # Append metrics for plotting\n",
        "    test_losses.append(avg_test_loss)\n",
        "    test_accuracies.append(acc)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Load the date and divede it -80% for testing,20% to training\n",
        "dataset = pd.read_csv(\"HateSpeechDatasetBalanced.csv\")\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=30)\n",
        "\n",
        "test_dataset = test_dataset.sample(n=10000, replace=False, random_state=30)\n",
        "\n",
        "# Load the tokenizer  of BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#create train,test objects\n",
        "train_dataset = Hate_speech(train_dataset, tokenizer)\n",
        "test_dataset = Hate_speech(test_dataset, tokenizer)\n",
        "\n",
        "#create DataLoader for train,test objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "Bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)  #default num of layers is 12\n",
        "model = FTB_Net(Bert_model).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=0.00005)\n",
        "\n",
        "\n",
        "# Lists to store training and testing metrics\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "num_epochs = 5\n",
        "model.load_state_dict(torch.load(\"FTBertTrained.pth\", map_location=device))\n",
        "\n",
        "test(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}